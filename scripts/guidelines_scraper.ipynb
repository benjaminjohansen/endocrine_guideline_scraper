{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define a translation table\n",
    "trans_table = str.maketrans(\"æøåÆØÅ\", \"aoaAOA\")\n",
    "\n",
    "# Trying with a dict instead\n",
    "translate_dict = {\n",
    "    \"æ\": \"ae\",\n",
    "    \"ø\": \"oe\",\n",
    "    \"å\": \"aa\",\n",
    "    \"Æ\": \"AE\",\n",
    "    \"Ø\": \"OE\",\n",
    "    \"Å\": \"AA\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Get all guideline links and from a collection. Also get the titel of the guideline\n",
    "2. For each guideline make a file named the same as the guideline seperated by _, and save all the content to the file.\n",
    "3. TODO: Make tests to check if there has been updates to the guidelines. Start a manual loop to figure out which guideline should be used. \n",
    "\n",
    "## Get all guidelins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_urls(url_to_scrape: str):\n",
    "    \"\"\"\n",
    "    Creates a list of urls from a website containing three elements\n",
    "\n",
    "    Input\n",
    "    url_to_scrape(str):             A url string to scrape\n",
    "\n",
    "    Output\n",
    "    url_header_filename_list(list): A list containing elements with three elements:\n",
    "                                    url:        of the subpage\n",
    "                                    header:     the header name of the list\n",
    "                                    filename:   the converted filename of the list in\n",
    "                                                as lower case, with underscores and with\n",
    "                                                Danish letters subbed with latin letters.\n",
    "    \"\"\"\n",
    "\n",
    "    assert url_to_scrape, \"You entered an empty string, please enter a valid url\"\n",
    "    main_url = url_to_scrape\n",
    "    # main_url = \"https://endocrinology.dk/nbv/diabetes-melitus/\"\n",
    "\n",
    "    main_response = requests.get(main_url)\n",
    "\n",
    "    main_soup = BeautifulSoup(main_response.content, \"html.parser\")\n",
    "\n",
    "    # Create a set to store the URLs. Set's are imutabel and great for the task.\n",
    "    url_set = set()\n",
    "\n",
    "    # Create a list to store the sets in\n",
    "    url_header_filename_list = []\n",
    "\n",
    "    # Select all <a> tags with class \"elementor-sub-item\"\n",
    "    links = main_soup.select(\"a.elementor-sub-item\")\n",
    "\n",
    "    # Now \"links\" is a list of all <a> tags with the class \"elementor-sub-item\"\n",
    "    # Iterate over each link and extract the url, the header and convert the header to\n",
    "    # a filename.\n",
    "    for link in links:\n",
    "        url = link.get(\"href\")\n",
    "        # The text describing the link will be the header for the document.\n",
    "        header = link.text  # or link.get_text()\n",
    "        # Convert the header text into snake_case\n",
    "        file_name = link.text.lower().replace(\" \", \"_\").replace(\".\", \"\")\n",
    "        # print(url, text)\n",
    "\n",
    "        # Clean up file names\n",
    "        for source, translate in translate_dict.items():\n",
    "            file_name = file_name.replace(source, translate)\n",
    "\n",
    "        # Limit the search to only contain the NBV's related to diabetes melitus\n",
    "        # Check if the URL starts with \"https://endocrinology.dk/nbv/diabetes-melitus/\"\n",
    "        # and is not equal to \"https://endocrinology.dk/nbv/diabetes-melitus/\"\n",
    "        if (\n",
    "            url.startswith(\"https://endocrinology.dk/nbv/diabetes-melitus/\")\n",
    "            and url != \"https://endocrinology.dk/nbv/diabetes-melitus/\"\n",
    "        ):\n",
    "            # Add the URL to the set\n",
    "            if url not in url_set:\n",
    "                url_set.add(url)\n",
    "                # print(url, header, file_name)\n",
    "                # Add the URL and text to the list\n",
    "                url_header_filename_list.append((url, header, file_name))\n",
    "\n",
    "    return url_header_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping each guideline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list(list_of_urls: list):\n",
    "    \"\"\"\n",
    "    This helper function test if a given list is a) not empty and b) contains three elements.\n",
    "    A warning is raised if there's empty elements in the list elements.\n",
    "    \"\"\"\n",
    "    # Assert that the list is not empty\n",
    "    assert list_of_urls, \"The list is empty.\"\n",
    "\n",
    "    # Assert that each item in the list has exactly three values and none of them are empty\n",
    "    for i, item in enumerate(list_of_urls):\n",
    "        assert len(item) == 3, f\"Row {i} does not have exactly three values.\"\n",
    "        empty_values = [j for j, value in enumerate(item, start=1) if not value]\n",
    "        assert (\n",
    "            not empty_values\n",
    "        ), f\"Row {i} has empty values at positions {empty_values}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_guidelines(list_of_guidelines: list, output_directory: str):\n",
    "    \"\"\"\n",
    "    Scrapes all guidelines from the Danish endocrine society given a list with URL's,\n",
    "    headers and file names.\n",
    "    Returns a file writen with the filenames.\n",
    "    input\n",
    "    list_of_guidelines(list):   A list containing three items:\n",
    "                                URL of the page\n",
    "                                The header as text (will be inserted into the document)\n",
    "                                The filename that the output file should be saved as\n",
    "    output_directory(str):      Where directory where the files are saved\n",
    "\n",
    "    output\n",
    "    A file      :   A file named after the filename containing all the text from a guideline.\n",
    "    \"\"\"\n",
    "    header_vars = [\"h1\", \"h2\", \"h3\"]\n",
    "\n",
    "    for guideline in list_of_guidelines:\n",
    "        # get the url from the guideline\n",
    "        url = guideline[0]\n",
    "        # create a html request\n",
    "        response = requests.get(url)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # output_file = \"../data/endocrinology_guidelines_type_2_diabetes.txt\"\n",
    "        output_file = f\"{output_directory}/{guideline[2]}.txt\"\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            # Write the header guideline as level 1\n",
    "            f.write(f\"#{guideline[1]}\\n\")\n",
    "            # Otherwise traverse through all headers and write to the file\n",
    "            for header in soup.find_all(header_vars):\n",
    "                for elem in header.next_siblings:\n",
    "                    if elem.name == header_vars[0]:\n",
    "                        f.write(f\"# {elem.get_text()}\\n\")\n",
    "                    elif elem.name == header_vars[1]:\n",
    "                        f.write(f\"## {elem.get_text()}\\n\")\n",
    "                    elif elem.name == header_vars[2]:\n",
    "                        f.write(f\"### {elem.get_text()}\\n\")\n",
    "                    # Write the paragraphs, if it is not the last paragraph, write it out\n",
    "                    elif (\n",
    "                        elem.name == \"p\"\n",
    "                        and elem.next_sibling\n",
    "                        and elem.next_sibling.name == \"p\"\n",
    "                    ):\n",
    "                        f.write(elem.get_text() + \"\\n\")\n",
    "                    else:\n",
    "                        f.write(elem.get_text() + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(list_of_guidelines)} guidelines to {output_directory}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 11 guidelines to ../data.\n"
     ]
    }
   ],
   "source": [
    "danish_endocrinology_url = \"https://endocrinology.dk/nbv/diabetes-melitus/\"\n",
    "url_header_filename_list = create_list_of_urls(danish_endocrinology_url)\n",
    "scrape_guidelines(url_header_filename_list, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the website\n",
    "url = \"https://endocrinology.dk/nbv/diabetes-melitus/behandling-og-kontrol-af-type-2-diabetes/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local header variables\n",
    "header_vars = [\"h1\", \"h2\", \"h3\"]\n",
    "output_file = \"../data/endocrinology_guidelines_type_2_diabetes.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for header in soup.find_all(header_vars):\n",
    "        for elem in header.next_siblings:\n",
    "            if elem.name == header_vars[0]:\n",
    "                f.write(f\"# {elem.get_text()}\\n\")\n",
    "            elif elem.name == header_vars[1]:\n",
    "                f.write(f\"## {elem.get_text()}\\n\")\n",
    "            elif elem.name == header_vars[2]:\n",
    "                f.write(f\"### {elem.get_text()}\\n\")\n",
    "            elif (\n",
    "                elem.name == \"p\" and elem.next_sibling and elem.next_sibling.name == \"p\"\n",
    "            ):\n",
    "                f.write(elem.get_text() + \"\\n\")\n",
    "            else:\n",
    "                f.write(elem.get_text() + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
